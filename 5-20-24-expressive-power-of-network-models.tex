\documentclass[letterpaper]{article}
\usepackage{notes}
\title{The Expressive Power of Neural Network Models}
\begin{document}

\maketitle

\section*{Problem Statement}

These days I've been trying to understand the bigger picture of our work on neural network models.  I'll start by asking some suggestive questions, to point us in the right direction. I've been wondering:

\begin{question}
    How do neural network models relate to other models for conditional \& modal logic?  What about the dynamics --- how do policies like Hebbian learning relate to belief revision policies such as lexicographic \& conservative upgrade?
\end{question}

\begin{question}
    The \href{http://flann.super.site/}{FLaNN Group}, specifically the work~\cite{merrill2019sequential,merrill2020formal,merrill2023expressive,strobl2024formal} considers neural networks as automata, and asks the question ``what functions can different neural networks encode?''  Similarly, we consider neural networks as models for logic, and ask the question ``what formulas can different neural networks model?''  These questions are clearly related --- but how, precisely?
\end{question}

\begin{question}
    The FLaNN perspective (neural networks as automata) is one way to characterize the computational power of neural networks.  Can we use neural network models to characterize the \emph{expressive} power of neural networks?  How does this all relate to the computational and descriptive complexity hierarchies?
\end{question}

\section*{Basic Setup and Definitions}

My first goal is to compare neural network models against other models.  To make the comparison fair, all models will share the basic multi-modal language
\[
    p \mid \neg \varphi \mid \varphi \land \psi \mid \set{\Box_i}_{i \in \textbf{I}} \hspace*{0.5em} \varphi
\]
where \textbf{I} is some fixed set of indices.  I have in mind that each $\Box_i$ represents a different modality per use-case (e.g. belief vs knowledge), although they could also be used to model a multi-agent setting (e.g. agent 1's belief vs agent 2's belief).

Here are some classes of models I'm interested in:

\paragraph*{Relational (Kripke) Models.} A relational model is $\Model = \langle W, \set{R_i}_{i \in \textbf{I}}, V \rangle$, where 
\begin{itemize}
    \item $W$ is some finite set of worlds (or states)
    \item Each $R_i \subseteq W \times W$ (the accessibility relations)
    \item $V : \textrm{Proposition} \to \mathcal{P}(W)$ (the valuation function)
\end{itemize}
Define $\Rel$ to be the class of all such models, and define $\Relrefl$ to be the class of all such models where each $R_i$ is additionally reflexive and transitive.  The semantics for both classes is given by:
\[
\begin{array}{lcl}
    \Model, w \Vdash p & \mbox{ iff } & w \in V(p)\\
    \Model, w \Vdash \neg \varphi & \mbox{ iff } & \Model, w \not \Vdash \varphi\\
    \Model, w \Vdash \varphi \land \psi & \mbox{ iff } & \Model, w \Vdash \varphi \mbox{ and } \Model, w \Vdash \psi\\
    \Model, w \Vdash \Box_i \varphi & \mbox{ iff } & \mbox{for all } u \mbox{ with } w{R_i}u, \Model, u \Vdash \varphi
\end{array}
\]

\paragraph*{Plausibility Models.}
A plausibility model is $\Model = \langle W, \set{R_i}_{i \in \textbf{I}}, V \rangle$, i.e. the models themselves are just relational models.  As before, I assume that $W$ is finite, and as with $\Relrefl$, $R_i$ is reflexive and transitive.  The key difference is that we interpret $\Box_i \varphi$ to hold in the best (or most plausible) states satisfying $\varphi$.  Formally, let $\best_{R_i}{(S)} = \set{w \in S \mid \textrm{For all } u \in S, \neg u {R_i} w}$ (the $R_i$-minimal states over $S$).  The new semantics for $\Box_i$ is
\[
\begin{array}{lcl}
    \Model, w \Vdash \Box_i \varphi & \mbox{ iff } & w \in \best_{R_i}{(\semantics{\varphi})}
\end{array}
\]
where $\semantics{\varphi} = \set{u \mid \Model, u \Vdash \varphi}$.  In practice, plausibility semantics coexist alongside relational semantics, so I allow some $\Box_i \varphi$ to be given relational semantics instead.  Let $\Plaus$ be the class of all such models.  Since we include relational operators, note that $\Relrefl \subseteq \Plaus$.

Any plausibility operator $\Box_i$ picks out a corresponding conditional: $\Box_i \varphi \to \psi$ reads ``the best $\varphi$ are $\psi$,'' which in the KLM tradition is the semantics for the conditional $\varphi \Rightarrow \psi$.

\paragraph*{Neighborhood Models.} A neighborhood model is $\Model = \langle W, \set{f_i}_{i \in \textbf{I}}, V \rangle$, where $W$ and $V$ are as before and each $f_i : W \to \mathcal{P}(\mathcal{P}(W))$ is an accessibility \emph{function}.  The intuition is that $f_i$ maps each state $w$ to the ``formulas'' (sets of states) that hold at $w$.  Define $\Nbhd$ to be the class of all neighborhood models.  

Moreover, the \emph{core} of $f$ is $\cap f(x) = \bigcap_{X \in f(w)} X$.  As with $\Rel$, let $\Nbhdrefl$ be the class of all neighborhood models that are additionally reflexive ($\forall w, w \in \cap f(w)$) and transitive ($\forall w$, if $X \in f(w)$ then $\set{v \mid X \in f(v)} \in f(w)$).

The semantics for both classes is the same as the previous classes, except the $\Box_i$ case is now:
\[
\begin{array}{lcl}
    \Model, w \Vdash \Box_i \varphi & \mbox{ iff } & \semantics{\varphi} \in f_i(w)
\end{array}
\]
where again $\semantics{\varphi} = \set{u \mid \Model, u \Vdash \varphi}$.

\paragraph*{Weighted Neural Network Models.} 
In our work \cite{kisby2024hebbian} so far, we've only defined the neural network state operators $\Prop$ and $\Reach$.  But in principle, we could define other closure operators, each reflecting a different kind of modality or conditional.  I want to characterize what a neural network  can \emph{in principle model}, and for this we need a more general definition.

A neural network model is $\Net = \langle N, \bias, \set{E_i}_{i \in \textbf{I}}, \set{W_i}_{i \in \textbf{I}}, \set{A}_{i \in \textbf{I}}, V \rangle$, where
\begin{itemize}
    \item $N$ is a finite nonempty set (the set of neurons)
    \item $\bias$ is a fixed node (the bias node)
    \item Each $E_i \subseteq N \times N$ (the edge relations)
    \item $W_i : E_i \to \mathbb{Q}$ (the weights for each edge relation)
    \item $A_i : \mathbb{Q} \to \mathbb{Q}$ (the activation function for each edge relation)
    \item $V : \textrm{Proposition} \to \mathcal{P}(N)$ (the valuation function)
\end{itemize}
I assume that each $A_i$ is a binary step function. Using the terminology of \cite{merrill2020formal}, this means the net is \emph{saturated}.  Later, I will assume one more constraint on the architecture of these nets.

A \emph{state} is just a possible activation pattern of the net. Since our activation functions $A_i$ are binary, either a neuron is active (1) or it is not (0).  So states are just binary sets of neurons. Additionally, the $\bias$ node is the only node active in every state. (Since it is active in every state, we can assume that no edges go into $\bias$.) Formally, we have
\[
    \State = \set{S \mid S \subseteq N \mbox{ and } \bias \in S}
\]

I've generalized the definition from previous papers: These neural network models can have multiple kinds of edges (indexed by $i \in \textbf{I}$) connecting the same nodes, along with their weights and a corresponding activation function for each $i$.  Each choice $E_i, W_i, A_i$ specifies a state transition function from state $S \in \mathcal{P}(W)$ to the next state, given by
\[
    F_i(S) = \set{n \mid A_i(\sum_{m \in \preds{n}} W_i(m, n) \cdot \bigchi_S(m)) = 1}
\]
where $\bigchi_S(m) = 1$ iff $m \in S$ is the indicator function.  In other words, $F_i(S)$ is the set of all nodes that are activated by their predecessors in $S$.

% I won't use this feature so much --- it's mainly to allow a fair comparison to the other classes of models.

% Second, I now allow different kinds of state transitions; each $F^i$ specifies how the net will use the edges, weights, and activation function to transition into the next state.  Specifically, $F^i_S(n)$ gives the activation of $n$ in the next state, given that the current state is $S$.  You can think of $E_i$, $W_i$, and $A_i$ as ``optional parameters'' that $F^i$ may make use of.

\begin{example*}
    Here are some choices of $E_i, W_i, A_i$ that give us common constructions for $F_i(S)$:
    \begin{enumerate}
        \item Given a graph $E_i$, pick 
        \[
            W_i(m, n) = 
            \begin{cases}
                1 & \mbox{if } m{E_i}n \\
                0 & \mbox{otherwise}
            \end{cases}
        \]
        Then pick $A_i(x) = 1$ iff $x > 0$. This gives us $n \in F_i(S)$ whenever at least one $E_i$-predecessor $m$ of $n$ is in $S$.  Call this the \emph{graph-reachability construction}.
        
        \item Given a graph $E_i$, pick $W_i(m, n) = \frac{1}{|\preds{n}|}$, and then pick $A_i(x) = 1$ iff $x \geq \frac{1}{2}$. Visually, for each node $n$ and its predecessors $m_1, \ldots, m_r$ we have
        \begin{center}
        \begin{tikzpicture}[loose/.style={inner sep=.  7em},edge/.style = {->,-Latex}, oval/.style={ellipse,draw}]
            %--------------------------------------------
            % Nodes
            \node[circle,fill,inner sep=2pt,label=left:$m_1$](a){};
            \node[below=0.75 of a,circle,fill,inner sep=2pt,label=left:$m_2$](b){};
            
            \node[below=0.5 of b](dots){$\rvdots$};

            \node[below=0.5 of dots,circle,fill,inner sep=2pt,label=left:$m_{r}$](c){};
            \coordinate (midway) at ($(a)!0.5!(c)$);
            \node[right=3 of midway,circle,fill,inner sep=2pt,label=right:$n$](n){};
            
            %--------------------------------------------
            % Excitatory edges
            \draw[edge, color=black, opacity=0.4] (a) -- (n) node [near start, above] {$\frac{1}{r}$};
            \draw[edge, color=black, opacity=0.4] (b) -- (n) node [near start, above] {$\frac{1}{r}$};
            \draw[edge, color=black, opacity=0.4] (c) -- (n) node [near start, above] {$\frac{1}{r}$};
        \end{tikzpicture}
        \end{center}
        This gives us $n \in F_i(S)$ if the majority (more than half) of $E_i$-predecessors are in $S$.  This is one of the choices that \cite{baltag2019socialnetworks} consider for modelling influence in social networks.
        
        \item Here is an interesting construction that Hannes uses in \cite{leitgeb2001nonmonotonic} to prove completeness for weighted neural network models.  He does this by way of inhibition nets, i.e. nets with inhibitory edges that block excitatory edges. I will give the full proof later, but for now the construction gives a taste of what weighted nets can do.
        
        Let $E_i$ be a given graph. First, create an edge from $\bias$ to every $n$ that is not $E_i$-minimal (in other words, if $n$ has any predecessors at all, then $\bias$ is one of them). Then for each node $n$ and its predecessors $\bias=m_0, m_1, \ldots, m_r$, connect inhibition edges as follows.
        
        \begin{center}
        \begin{tikzpicture}[loose/.style={inner sep=.  7em},edge/.style = {->,-Latex}, oval/.style={ellipse,draw}]
            %--------------------------------------------
            % Nodes
            \node[circle,fill,inner sep=2pt,label=left:$\bias$](a){};
            \node[below=0.75 of a,circle,fill,inner sep=2pt,label=left:$m_1$](b){};
            \node[below=0.75 of b,circle,fill,inner sep=2pt,label=left:$m_2$](c){};
            
            \node[below=0.5 of c](dots){$\rvdots$};

            \node[below=0.5 of dots,circle,fill,inner sep=2pt,label=left:$m_{r-1}$](d){};
            \node[below=0.75 of d,circle,fill,inner sep=2pt,label=left:$m_r$](e){};
            
            \coordinate (midway) at ($(a)!0.5!(e)$);
            \node[right=3 of midway,circle,fill,inner sep=2pt,label=right:$n$](n){};
            
            % Draw the inhibitory edge that crosses over first
            \draw[edge, -Circle, color=black, opacity=0.4] (e) -- ($(a)!0.35!(n)$) node [near start, above] {};

            %--------------------------------------------
            % Excitatory edges
            % draw white over each of the middle ones to
            % erase the crossover
            \draw[edge, color=black, opacity=0.4] (a) -- (n) node [near start, above] {};
            \draw[edge, color=white, opacity=1, line width=3pt, -, shorten >=10pt, shorten <=4pt] (b) -- (n) node [near start, above] {};
            \draw[edge, color=black, opacity=0.4] (b) -- (n) node [near start, above] {};
            \draw[edge, color=white, opacity=1, line width=3pt, -, shorten >=10pt, shorten <=4pt] (c) -- (n) node [near start, above] {};
            \draw[edge, color=black, opacity=0.4] (c) -- (n) node [near start, above] {};
            \draw[edge, color=white, opacity=1, line width=3pt, -, shorten >=10pt, shorten <=4pt] (d) -- (n) node [near start, above] {};
            \draw[edge, color=black, opacity=0.4] (d) -- (n) node [near start, above] {};
            \draw[edge, color=black, opacity=0.4] (e) -- (n) node [near start, above] {};

            %--------------------------------------------
            % Inhibitory edges
            % draw white over each of them to erase the crossover
            \draw[edge, color=white, opacity=1, line width=3pt, -, shorten >=10pt, shorten <=10pt] (a) -- ($(b)!0.5!(n)$) node [near start, above] {};
            \draw[edge, -Circle, color=black, opacity=0.4] (a) -- ($(b)!0.5!(n)$) node [near start, above] {};
            
            \draw[edge, color=white, opacity=1, line width=3pt, -, shorten >=10pt, shorten <=4pt] (b) -- ($(c)!0.5!(n)$) node [near start, above] {};
            \draw[edge, -Circle, color=black, opacity=0.4] (b) -- ($(c)!0.5!(n)$) node [near start, above] {};
            
            \node[right=1 of dots,yshift=0.5em](dotsb){$\rvdots$};

            \draw[edge, color=white, opacity=1, line width=3pt, -, shorten >=10pt, shorten <=4pt] (d) -- ($(e)!0.5!(n)$) node [near start, above] {};
            \draw[edge, -Circle, color=black, opacity=0.4] (d) -- ($(e)!0.5!(n)$) node [near start, above] {};
        \end{tikzpicture}
        \end{center}
        
        That is, each node $m_i$ is inhibited by $m_{i-1}$ ($\bias=m_0$ inhibited by $m_r$).  This has the following effect: if all $m_i$ activate, they each inhibit each other, and so $n$ does not activate.  If only \emph{some} $m_i$ activate, then there is some $m_i$ that is uninhibited, and so $n$ activates.  And finally, since $\bias$ is always active we cannot have \emph{no} $m_i$ active.  In other words, $n \in F_i(S)$ iff \emph{not all} predecessors $m_i$ are in $S$.
        
        We can simulate this effect with weighted neural networks. Create an edge from $\bias$ to every $n$ that is not $E_i$-minimal. Then pick $W_i(m, n) = \frac{1}{|\preds{n}|+1}$ (the extra $+1$ accounts for the $\bias$). Finally, pick $A_i(x) = 1$ iff $x < 1$.  Take a moment to check that $n \in F_i(S)$ iff not all predecessors $m_i$ are in $S$.  Call this the \emph{not-every construction}.
    \end{enumerate}
\end{example*}

Let $\NetModels$ be the class of all neural network models (with one more constraint that I'll say soon).  The semantics for $\Net \in \NetModels$ is a bit more roundabout than for the previous classes.  First, we define a ``next state'' function $\nextstate_i : \mathcal{P}(N) \to \mathcal{P}(N)$ for each $i \in \textbf{I}$ as follows:
\[
    \nextstate_i(S) = S \cup F_i(S)
\]
Notice that $\nextstate(S)$ is extensive --- once activated, a node will stay activated in the next state.

\begin{postulate*}
    I assume that for all $i \in \textbf{I}$ and all states $S$, $\nextstate_i$ applied to $S$, i.e.
    \[
        S, \nextstate_i(S), \nextstate_i(\nextstate_i(S)), \ldots, \nextstate^k_i(S), \ldots
    \]
    has a (finite) least fixed point, and moreover that it is \emph{unique}.  Let $\Closure_i : \mathcal{P}(N) \to \mathcal{P}(N)$ (``closure'') be the function that produces that least fixed point.  For concreteness, we can say that there is some $k\in \mathbb{N}$ for which
    \[
        \Closure_i(S) = \nextstate_i^k(S)
    \]
\end{postulate*}
This assumption implicitly constrains the allowed neural network architectures: We allow feed-forward nets, as well as certain controlled forms of recurrence.  Characterizing nets that have a unique least fixed point is a big open problem.

\begin{example*}
    In general, $\Closure_i(S)$ is exactly $\Prop(S)$, the forward propagation (or diffusion) of the signal $S$ through the net.  But different constructions for $F_i(S)$ give different interpretations for $\Closure(S)$.  
    
    Consider the constructions from the previous example.  The graph-reachability construction gives us $\Closure_i(S) = \Reach(S)$, the nodes graph-reachable from $S$.  Similarly, $\Closure_i$ for the social majority construction can be interpreted as the diffusion of an opinion or attitude through a social network (see \cite{baltag2019socialnetworks}).
\end{example*}

I can now state the semantics for $\Net \in \NetModels$:
\[
\begin{array}{lcl}
    \Net, n \Vdash p & \mbox{ iff } & n \in V(p)\\
    \Net, n \Vdash \neg \varphi & \mbox{ iff } & \Net, n \not \Vdash \varphi\\
    \Net, n \Vdash \varphi \land \psi & \mbox{ iff } & \Net, n \Vdash \varphi \mbox{ and } \Model, n \Vdash \psi\\
    \Net, n \Vdash \Diamond_i \varphi & \mbox{ iff } & 
    n \in \Closure_i(\semantics{\varphi})
\end{array}
\]
where $\semantics{\varphi} = \set{n \mid \Net, n \Vdash \varphi}$.  

Any network diffusion operator $\Diamond_i$ picks out a corresponding neural network inference: $\Diamond_i \varphi \leftarrow \psi$ says that on input $\varphi$ the neural network ``answers'' with classification $\psi$.  This is analogous to the way a plausibility operator picks out a conditional (I will say more about this later).

\paragraph*{Dynamic Models.}

I would also like to compare neural network \emph{update} against other model updates --- what kind of updates are neural networks capable of modelling, and how expressive are they in this sense?  We can ``dynamify'' each of the models above using the dynamic epistemic logic trick.  First, we extend our language to include dynamic modal operators:
\[
    p \mid \neg \varphi \mid \varphi \land \psi \mid \set{\Box_i}_{i \in \textbf{I}} \hspace*{0.5em} \varphi \mid \set{\Update{\varphi}_j}_{j \in \textbf{J}} \hspace*{0.5em} \psi
\]
where \textbf{J} is a new set of indices.  As before, the idea is that each $\Update{\varphi}_i$ represents a different update per use-case, although taking $\textbf{I} = \textbf{J}$ these could also be used to model different updates per agent.  

I will define the semantics for dynamic models by example.  First, consider $\Rel$.  For any $\Model \in \Rel$ and $S \in \mathcal{P}(W)$, we can define a variety of dynamic update functions $\set{\UpdateVerbatim_j}_{j \in \textbf{J}}$, where each $\UpdateVerbatim_j : \Rel \times \mathcal{P}(W) \to \Rel$.  A classical example is public announcement, where $\UpdateVerbatim(\langle W, R, V \rangle, S) = \langle W \cap S, R, V \rangle$.  Note that we're using sets $S$ as input rather than formulas, although the choice of one over the other is just my preference.

Also note that these updates don't depend on the current world $w$.  I've read a good bit of \cite{van2011logicaldynamics} (a great book on dynamic epistemic logics), and while Johan includes dependence on $w$ I haven't yet run across an update that uses this extra information.  I'll drop it for now, but if we need it later I can add it back in.

The semantics for $\Model \in \Rel$ over the dynamic language is exactly as before, with an additional case for $\Update{\varphi}_j \psi$.  We just interpret $\Update{\varphi}_j \psi$ as ``$\psi$ holds after updating by $\varphi$'':
\[
\begin{array}{lcl}
    \Model, w \Vdash \Update{\varphi}_j \psi & \mbox{ iff } & \Model^\star_{\semantics{\varphi}}, w \Vdash \psi
\end{array}
\]
We make the same move for each of the other model classes.  For all classes $\Class$, define $\Class^\star$ to be the class of all such models with the new semantics over the dynamic language (e.g. $\Rel^\star$).

% A dynamic relational model is $\Model' = \langle W, \set{R_i}_{i \in \textbf{I}}, \set{\UpdateVerbatim_j}_{j \in \textbf{J}}, V \rangle$, where each $\UpdateVerbatim_j : \Rel \times \mathcal{P}(W) \to \Rel$ is a dynamic update function that produces a new model given a set $S \in \mathcal{P}(W)$.  Given $\Model \in \Rel$ and $S \in \mathcal{P}(W)$, I'll write $\Model_S^{\star_j} = \UpdateVerbatim_j(\Model, S)$ as shorthand.  Define $\Rel^\star$ to be the class of all such models.

% $\Plaus^\star$, $\Nbhd^\star$, and $\NetModels^\star$ are defined similarly --- just extend the model with update functions $\UpdateVerbatim_j$ and the semantics are the same.


% P ] is some dynamic update given by M → M⋆
% P (this is a free variable; the problem will be to find the right update).

\section*{Measuring Expressive Power.} 

To compare the expressive power of neural networks with other logic models, I need to pick a measure of complexity.  I would like to focus on the question ``what formulas can a neural network architecture in-principle model (satisfy)?'' as a proxy for ``what functions can a neural network architecture in-principle compute?''

\begin{definition}
    Let $\Model$ be any model whatsoever with universe $W$ and satisfaction relation $\Vdash$.  $\Model \models \varphi$ if for all $w \in W$, $\Model, w \Vdash \varphi$.
\end{definition}

\begin{definition}
    Let $\Class$ be a class of models, $\Model$ be a model in $\Class$, and $\lang$ be a language.  The \emph{formulas satisfiable by} $\Model$ over $\lang$ is given by $\Satisfy{\Model} = \set{\varphi \in \lang \mid \Model \models \varphi}$.  The \emph{formulas satisfiable by class $\Class$} over $\lang$ is given by 
    \[
        \Satisfy{\Class} = \set{\varphi \in \lang \mid \textrm{there is some } \Model \in \Class \textrm{ such that } \Model \models \varphi}
    \]
\end{definition}

\paragraph*{Note.}  I want to point out two things.  First, $\Satisfy{\Class}$ is the dual of the \emph{theory} of $\Class$, i.e.
\[
    \Theory{\Class} = \set{\varphi \in \lang \mid \textrm{for all } \Model \in \Class \textrm{ we have } \Model \models \varphi}
\]
It follows from duality that $\Satisfy{\Class_1} \subseteq \Satisfy{\Class_2}$ iff $\Theory{\Class_2} \subseteq \Theory{\Class_1}$.  In this sense, these two operators are just two different perspectives on the expressive power of a model.  Model theorists tend to prefer studying $\Theory{\Class}$.  I personally prefer $\Satisfy{\Class}$, since it puts less expressive models on the bottom and more expressive ones on top.

Second, $\Satisfy{\Class}$ is different from the measure of expressive power normally used by descriptive complexity theorists.  Descriptive complexity focuses on the \emph{properties definable in $\lang$}, i.e.
\[
    \Definable{\Class} = \set{P \mid \textrm{there exists } \varphi \in \lang \textrm{ such that for all } \Model \in \Class, \Model \in P \textrm{ iff } \Model \models \varphi}
\]
This is an important distinction between my focus and the focus of descriptive complexity.  Descriptive complexity compares the expressive power of different \emph{languages} when we keep their \emph{models} fixed -- definability is an appropriate measure for that.  But I want to look at the expressive power of different \emph{models} when we keep their \emph{languages} fixed -- I believe satisfiability is a good measure for this.

\begin{example*}
    Here's an example that's a sort of tutorial for comparing the expressive power of different model classes using $\Satisfy{\Class}$.  Consider relational models $\Rel$ over the static language.  Here are some formulas that are valid in every relational model:
    \begin{itemize}
        \item $\Box_i(\varphi \land \psi) \to (\Box_i \varphi \land \Box_i \psi)$
        \item $(\Box_i \varphi \land \Box_i \psi) \to \Box_i(\varphi \land \psi)$
        \item $\Box_i \top$
    \end{itemize}
    This means that \emph{no} $\Model \in \Rel$ can satisfy their negations (for details, see \cite{pacuit2017neighborhood}, page 8).  But neighborhood models can: Let $\Model = \langle W, f, V \rangle$ be a neighborhood model with $W = \set{a, b, c}$, propositions $\set{p, q}$ with $V(p) = \set{a, b}, V(q) = \set{b, c}$, and $f$ given by
    \[
    \begin{tikzcd}[column sep=1em]
        \set{a, c} & \set{c} & \set{b} & \emptyset & \set{a} & \set{b,c} & \set{b} \\
           &  a \arrow{lu} \arrow{u} \arrow{ru}  &     &  b \arrow{lu} \arrow{u} \arrow{ru}  &    &  c \arrow{lu} \arrow{u} \arrow{ru} &
    \end{tikzcd}
    \]
    For all $w \in W$ we have $\semantics{p} \cap \semantics{q} = \set{b} \in f(w)$, but also $\semantics{p} = \set{a, b} \not \in f(w)$. This means that for all $w$, $\Model, w \not \Vdash \Box(\varphi \land \psi) \to (\Box \varphi \land \Box \psi)$ --- in other words, $\Model \models \neg (\Box(\varphi \land \psi) \to (\Box \varphi \land \Box \psi))$.  Consequently, this formula is in $\Satisfy{\Nbhd}$, but \emph{not} in $\Satisfy{\Rel}$.
    
    Moreover, neighborhood models are at least as expressive as relational models: $\Satisfy{\Rel} \subseteq \Satisfy{\Nbhd}$.  Let $\varphi \in \lang$ and suppose $\Model \models \varphi$ for some $\Model = \langle W, \set{R_i}_{i \in \textbf{I}}, V \rangle \in \Rel$.  Eric Pacuit in \cite{pacuit2017neighborhood}, page 47 shows how to construct an equivalent neighborhood model:  Let $\Model' = \langle W, \set{f_i}_{i \in \textbf{I}}, V \rangle$, where each $f_i(w) = \set{X \mid \set{v \mid w{R_i}v } \subseteq X}$.  All we need to show is $\Model' \models \varphi$, but we are able to prove something even stronger:
    \begin{claim*}
        For all $\varphi$ and all $w$, $\Model, w \Vdash \varphi$ iff $\Model', w \Vdash \varphi$.
    \end{claim*}
    \begin{proof}
        By induction on $\varphi$.  The key case is $\Box_i \varphi$.  
        
        $(\to)$: Suppose $\Model, w \Vdash \Box_i \varphi$, i.e. for all $u$ with $w{R_i}u$, $\Model, u \Vdash \varphi$.  It follows that $\set{u \mid w{R_i}u } \subseteq \semantics{\varphi}$. By construction of $\Model'$, this means $\semantics{\varphi} \in f_i(w)$, and so $\Model', w \Vdash \Box_i \varphi$.

        $(\leftarrow)$: Now suppose $\Model', w \Vdash \Box_i \varphi$, i.e. $\semantics{\varphi} \in f_i(w)$.  By construction of $\Model'$, we have $\set{u \mid w{R_i}u } \subseteq \semantics{\varphi}$.  So for all $u$ with $w{R_i}u$, $\Model, u \Vdash \varphi$.  This gives us $\Model, w \Vdash \Box_i \varphi$.
    \end{proof}

    Putting everything together, neighborhood models are \emph{strictly} more expressive than relational models:
    \begin{proposition}
        $\Satisfy{\Rel} \subset \Satisfy{\Nbhd}$
    \end{proposition}

\end{example*}


\section*{Known Results (The Static Case)}

Recall the first part of Question 1: ``how do neural network models relate to other models for conditional \& modal logic?'' The static case is already understood. I'll collect the known results in this section.

We can see how the expressive power of neural network models compares against known models in logic by arranging their satisfiable sets in an ``expressivity hierarchy.''  To make the comparison with neural networks fair, I will only consider the reflexive and transitive variants $\Relrefl, \Nbhdrefl$ of relational and neighborhood models.  Over the static language (no dynamic operators), we have:
\[
\Satisfy{\Relrefl} \subset \Satisfy{\Plaus} = \Satisfy{\NetModels} \subset \Satisfy{\Nbhdrefl}
% \begin{tikzcd}
%     & \Satisfy{\Nbhd} \arrow[symbol=\supset]{ld} \arrow[symbol=\supset]{rd} & & \\
%     \Satisfy{\Rel} \arrow[symbol=\supset]{rd} & & \Satisfy{\Plaus} \arrow[symbol={=}]{r} \arrow[symbol=\supset]{ld} & \Satisfy{\NetModels} \\
%     & \Satisfy{\Relrefl} & &
%     % A \arrow[r,symbol=\supseteq] &B \arrow[d] \\
%     % & G \arrow[r,symbol=\leq] & H
% \end{tikzcd}
\]

The first and last inclusions are folklore.  I haven't found a reference yet, but I was able to prove them by following the motions of our previous example.

\begin{proposition}
    $\Satisfy{\Relrefl} \subset \Satisfy{\Plaus}$
\end{proposition}
\begin{proof}
    First, since I allowed reflexive, transitive relational operators in plausibility models, we immediately have $\Relrefl \subseteq \Plaus$.  This means that, given any $\Model \in \Relrefl$ satisfying $\varphi$, the very same model is $\in \Rel$.  As for strictness, nonmonotonicity $\neg (\Box(\varphi \land \psi) \to (\Box \varphi \land \Box \psi)) \in \Satisfy{\Plaus}$, yet it is not satisfiable by $\Model \in \Relrefl$.
\end{proof}

% These inclusions are all either already known or are folklore.  First, the easy ones:
% \begin{itemize}
%     \item $\Satisfy{\Rel} \subset \Satisfy{\Nbhd}$: We already proved this in the example above.
    
%     \item $\Satisfy{\Relrefl} \subset \Satisfy{\Rel}$: Given any $\Model \in \Relrefl$ satisfying $\varphi$, the very same model is $\in \Rel$.  As for the strictness, the negation of the reflexivity axiom $\neg (\Box{\varphi} \to \varphi) \in \Satisfy{\Rel}$, yet $\neg (\Box{\varphi} \to \varphi) \not \in \Satisfy{\Relrefl}$.
    
%     \item $\Satisfy{\Relrefl} \subset \Satisfy{\Plaus}$: First, since I allowed relational operators in plausibility models, we immediately have $\Relrefl \subseteq \Plaus$.  This means that given any $\Model \in \Relrefl$ satisfying $\varphi$, the very same model is $\in \Rel$.  As for strictness, nonmonotonicity $\neg (\Box(\varphi \land \psi) \to (\Box \varphi \land \Box \psi)) \in \Satisfy{\Plaus}$, yet it is not satisfiable by $\Model \in \Relrefl$.
    
%     \item Neither $\Satisfy{\Rel}$ nor $\Satisfy{\Plaus}$ are contained in the other:  On the one hand, nonmonotonicity $\neg (\Box (\varphi \land \psi) \to \Box \varphi \land \Box \psi)$ is satisfiable by $\Model \in \Plaus$ but not by $\Model \in \Rel$.  On the other hand, the negation of the reflexivity axiom $\Box \varphi \to \varphi$ is satisfiable by $\Model \in \Rel$, but reflexivity and transitivity hold in all plausibility models (we could have also taken the corresponding transitive or cumulative axioms).  Because of this, it's more helpful to compare $\Satisfy{\Plaus}$ with $\Satisfy{\Relrefl}$.
% \end{itemize}

% Now for the more interesting inclusions:

\begin{proposition}
    $\Satisfy{\Plaus} \subset \Satisfy{\Nbhdrefl}$
\end{proposition}
\begin{proof}
    First, let's show inclusion.  Let $\Model = \langle W, \set{R_i}_{i \in \textbf{I}}, V \rangle$ be a plausibility model satisfying $\varphi$.  We construct the neighborhood model $\Model' = \langle W, \set{f_i}_{i \in \textbf{I}}, V \rangle$, where $f_i$ is given as follows.  If $\Box_i$ is given relational semantics, then $f_i(w) = \set{X \mid \set{v \mid w{R_i}v }\subseteq X}$ as in our example above.  But if $\Box_i$ has plausibility semantics,
    \[
        f_i(w) = \set{X \in \mathcal{P}(W) \mid w \in \best_{R_i}(X)}
    \]
    First, we need to check that this is in fact in $\Nbhdrefl$, i.e. these choices of $f_i$ are reflexive and transitive.
    \begin{description}
        \item[$f_i$ is reflexive (relational case).]
        We want to show that for all $w$, $w \in \cap f_i(w)$.  First, since $R_i$ is reflexive, for all $w$, $w{R_i}w$.  So $w \in \set{u \mid w{R_i}u}$.  But this means $\set{u \mid w{R_i}u}$ implies $w \in X$.  Applying our choice of $f_i$, we have $X \in f(w)$ implies $w \in X$.  This immediately gives $w \in \bigcap_{X \in f_i(w)} X = \cap f_i(w)$.

        \item[$f_i$ is transitive (relational case).]
        We want to show that for all $w$ and $X$, if $X \in f_i(w)$ then $\set{u \mid X \in f_i(u)} \in f_i(w)$.  Suppose that $X \in f_i(w)$.  By definition, $\set{u \mid w{R_i}u} \subseteq X$.  Now let $u, v$ be arbitrary, and suppose $w{R_i}u, u{R_i}v$.  Since $R_i$ is transitive, $w{R_i}v$.  But then $v \in \set{u \mid w{R_i}u} \subseteq X$, so $v \in X$.  Since $u$ and $v$ were chosen arbitrarily, in general we have
        \[
            \set{u \mid w{R_i}u} \subseteq \set{u \mid \set{v \mid u{R_i}v} \subseteq X}
        \]
        But by choice of $f_i$, this is exactly $\set{u \mid X \in f(u)} \in f(w)$.

        \item[$f_i$ is reflexive (plausibility case).]
        We want to show that for all $w$, $w \in \cap f_i(w)$.  First, for all sets $X$ we have $\best_i(X) \subseteq X$.  In other words, $w \in \best_i(X)$ implies $w \in X$.  So $X \in f_i(w)$ implies $w \in X$.  But this immediately gives $w \in \bigcap_{X \in f_i(w)} X = \cap f_i(w)$.

        \item[$f_i$ is transitive (plausibility case).]
        We want to show that for all $w$ and $X$, if $X \in f_i(w)$ then $\set{u \mid X \in f_i(u)} \in f_i(w)$.  Suppose that $X \in f_i(w)$.  By definition, $w \in \best_i(X)$.  But by idempotence of $\best$, $w \in \best_i(\best_i(X))$.  Applying our choice of $f_i$ to the inner $\best$, we get $w \in \best_i(\set{u \mid X \in f(u)})$.  Applying the definition once more, we have $\set{u \mid X \in f(u)} \in f(w)$.

    \end{description}

    Next, we will show that for all $\varphi, w$, $\Model, w \Vdash \varphi$ iff $\Model', w \Vdash \varphi$.  We do this by induction on $\varphi$.  The key inductive case is $\Box_i \varphi$.  The relational case is handled by the example in the previous section.  For plausibility operators $\Box_i$, we have
    \[
    \begin{array}{lcl}
        \Model, w \Vdash \varphi & \mbox{ iff } & w \in \best_{R_i}(X) \\
        & \mbox{ iff } & X \in f_i(w) \\
        & \mbox{ iff } & \Model', w \Vdash \Box_i \varphi
    \end{array}
    \]
    We conclude that for our particular $\varphi$, $\Model \models \varphi$ implies $\Model' \models \varphi$.

    Strictness is easy: nonreflexivity $\neg (\Box \varphi \to \varphi) \in \Satisfy{\Nbhd}$, but $\neg (\Box \varphi \to \varphi) \not \in \Satisfy{\Plaus}$ (since both relational \emph{and} plausibility operators are always reflexive).
\end{proof}

Let's move on to the neural network inclusions.  I claim that $\Satisfy{\NetModels} = \Satisfy{\Plaus}$: neural network and plausibility models are equally expressive (up to inference in the static language).  The $(\to)$ direction follows from Hannes' completeness result \cite{leitgeb2001nonmonotonic, leitgeb2003nonmonotonic} --- he uses the \emph{not-every construction} from before to build a neural net from a plausibility model.  The $(\leftarrow)$ direction is essentially the soundness result \cite{leitgeb2001nonmonotonic, leitgeb2003nonmonotonic}. But explicitly building a plausibility model from a neural net has not been done, as far as I know.  So the proof of $(\leftarrow)$ is my own.

% It is known that neural network models and plausibility models are both complete with respect to the proof system of cumulative conditionals \cite{leitgeb2001nonmonotonic, leitgeb2003nonmonotonic} -- the fact that $\Satisfy{\NetModels} = \Satisfy{\Plaus}$ should follow.  But (1) Hannes shows how to construct a net from a plausibility model, but not the other way around, and (2) my definitions are slightly different from Hannes'.  So I will re-prove this result, adapted for my setting, as a sort of sanity check.

\begin{proposition}
    $\Satisfy{\Plaus} \subseteq \Satisfy{\NetModels}$
\end{proposition}
\begin{proof}
    Let $\Model = \langle W, \set{R_i}_{i \in \textbf{I}}, V \rangle$ be a plausibility model satisfying $\varphi$.  We construct the neural network model $\Net = \langle N, \bias, \set{E_i}_{i \in \textbf{I}}, \set{W_i}_{i \in \textbf{I}}, \set{A_i}_{i \in \textbf{I}}, V \rangle$ as follows.  First, let $N = W$, let $\bias$ be a fresh node, and keep the propositional evaluation $V$ the same. \textbf{TODO}

    We have two cases:
    \begin{description}
        \item[$\Box_i$ is given relational semantics.] 
        
        \item[$\Box_i$ is given plausibility semantics.]  
    \end{description}
\end{proof}

\begin{proposition}
    $\Satisfy{\NetModels} \subseteq \Satisfy{\Plaus}$
\end{proposition}


% \begin{proof}
%     As for $\Satisfy{\NetModels} = \Satisfy{\Plaus}$, Hannes in \cite{leitgeb2001nonmonotonic, leitgeb2003nonmonotonic} proved this for particular classes of plausibility models and networks over a conditional language.  Since my models are somewhat more general, and I use a modal language, I just need to lift his result to the new setting:
%     sorry
% \end{proof}

% \paragraph*{What does this say about interpreting neural networks?}
% The popular conception of neural networks is that they are ``black boxes'' whose behavior is difficult to understand.  This last fact tells us, however, that neural network inference (classification) corresponds precisely to defeasible conditionals.  When Thomas Icard last visited, he pointed out to me that this intuition is only helpful for describing the net's input/output behavior, but not for explaining the net's hidden states.  Because of this, Thomas thinks that the ``neural network model'' formalism is on the wrong track (or at least, unenlightening), and he decided not to continue working on it.

% I take this criticism seriously, but to me the underlying issue is that the hidden states are determined through learning, and we don't yet have a classical intuition for neural network learning.  If we did, then we could understand and track the changes in the hidden states during the learning process, and there would be no need for extraction.  I'll revisit this point at the end of the next section.

\section*{Progress So Far (The Dynamic Case)}

The dynamic case is much more interesting.  For a class of models $\Class$, $\Satisfy{\Class^\star}$ captures those dynamic updates that $\Class$ is capable of modelling.  This gets at the second part of Question 1: What updates are possible over neural networks?  What updates are possible over plausibility models?  What is the relationship between neural network updates and plausibility model updates?

I'm not aware of any work that compares different model classes' capacity to model different dynamic updates --- this is unexplored turf.  I'll take a first stab by \textbf{conjecturing} the following:
\[
\begin{tikzcd}
    & \Satisfy{\Nbhdrefl^\star} \arrow[symbol=\supset]{ld} \arrow[symbol=\supset]{rd} & \\
    \Satisfy{\Plaus^\star} \arrow[symbol=\supset]{rd} \arrow[dash]{r} & \circled{?} & \Satisfy{\NetModels^\star} \arrow[symbol=\supset]{ld} \arrow[dash]{l} \\
    & \Satisfy{\Relrefl^\star} & 
\end{tikzcd}
\]
I expect that the outermost inclusions are going to be easy(ish).  The crucial inclusion is $\circled{?}$, which I'm not willing to conjecture on yet.  But the situation is interesting, since there are exactly four possibilities, all of which are exciting:
\begin{description}
    \item[$\Satisfy{\Plaus^\star} = \Satisfy{\NetModels^\star}$.] If the two are equal, this work creates a bridge between plausibility models and neural networks.  It would suggest that two groups of mostly independent researchers arrived at the same representation for learning.  And in this case, it would be nice to explicitly see (1) what neural network updates radical and conservative upgrade correspond to, and (2) what ``classical'' plausibility updates correspond to network policies such as Hebbian learning or backpropagation.
    \item[$\Satisfy{\Plaus^\star} \subset \Satisfy{\NetModels^\star}$.] In this case, we discover that neural networks are strictly more powerful than plausibility models: for every plausibility upgrade there is a neural network equivalent, but not the other way around.  What kind of neural network policies cannot be represented as plausibility upgrade?  Is this what makes Hebbian learning or backpropagation special?  This possibility offers a theoretical justification for why we use neural networks in machine learning rather than plausibility models, and perhaps why classical models historically have not had the same kind of success as neural networks.
    \item[$\Satisfy{\Plaus^\star} \supset \Satisfy{\NetModels^\star}$.] This flips the previous scenario, and perhaps goes counter to common wisdom.  Instead we discover that plausibility models can represent any kind of update neural networks can.  What kind of plausiblity updates can't be represented with a neural network?  This case offers a new direction for machine learning researchers to look for new learning algorithms, and suggests that they should adopt a different model. 
    \item[$\Satisfy{\Plaus^\star}$ and $\Satisfy{\NetModels^\star}$ are incomparable.] This is the case that I personally believe is most likely.  As before, we can ask: what specific update policies aren't representable with the other model?  But we see that neither plausibility models nor neural networks are as rich as we would like them to be.  This puts us in the difficult --- but exciting! --- position of finding a model class $\Class$ that can model both types of update.  Of course, $\Nbhdrefl^\star$ will work, but it would be nice to find a tighter bound.
\end{description}

Okay, enough waxing philosophical. Let's get to work. A first observation is that we inherit inequalities from the static case.  This means that, for the outer inclusions, we only need to show $\subseteq$ (we get $\ne$ for free).
\begin{lemma}
    For all classes of models $\Class_1, \Class_2$, if $\Satisfy{\Class_1} \ne \Satisfy{\Class_2}$ then $\Satisfy{\Class_1^\star} \ne \Satisfy{\Class_2^\star}$.
\end{lemma}
\begin{proof}
    \textbf{TODO}
\end{proof}

Now let's tackle the outer inclusions:
\begin{proposition}
    $\Satisfy{\Relrefl^\star} \subset \Satisfy{\Plaus^\star}$
\end{proposition}
\begin{proof}
    \textbf{TODO}
\end{proof}

\begin{proposition}
    $\Satisfy{\Plaus^\star} \subset \Satisfy{\Nbhdrefl^\star}$
\end{proposition}
\begin{proof}
    \textbf{TODO}
\end{proof}

\begin{proposition}
    $\Satisfy{\Relrefl^\star} \subset \Satisfy{\NetModels^\star}$
\end{proposition}
\begin{proof}
    \textbf{TODO}
\end{proof}

\begin{proposition}
    $\Satisfy{\NetModels^\star} \subset \Satisfy{\Nbhdrefl^\star}$
\end{proposition}
\begin{proof}
    \textbf{TODO}
\end{proof}



\section*{Todo List}
\begin{todolist}
    % \item Prove that the model we construct in $\Nbhdrefl$ is actually reflexive and transitive.
    % \item Re-write neural network section -- no more general transition functions, instead give a list of common constructions (how we can simulate different transition/accessibility functions).
    \item Prove that $\Satisfy{\Plaus} \subseteq \Satisfy{\NetModels}$ by construction.
    \item Prove that $\Satisfy{\NetModels} \subseteq \Satisfy{\Plaus}$ by construction.
    \item Write out the dynamic case (dump my thoughts out for starters)
\end{todolist}

% \subsection*{Neural Networks and the Complexity Hierarchy}

% [Include big picture relating Chomsky hierarchy, neural networks as automata (Will Merill's work), and descriptive complexity, done in Inkscape]

% \subsubsection*{The Dynamic Case.}

% [All of this suggests a whole research program --- identify important classes of \emph{dynamic updates} and explore the \emph{dynamic} complexity hierarchy! Say what this means for automata, neural networks, and logic expressivity.  Open problems abound, this is very unexplored territory!]

\printbibliography
\end{document}

