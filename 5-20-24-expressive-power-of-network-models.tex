\documentclass[letterpaper]{article}
\usepackage{notes}
\begin{document}

\section*{Problem Statement}

These days I've been trying to understand the bigger picture of our work on neural network models.  I'll start by asking some suggestive questions, to point us in the right direction. I've been wondering:

\begin{question}
    How do neural network models relate to other models for conditional \& modal logic?  What about the dynamics --- how do policies like Hebbian learning relate to belief revision policies such as lexicographic \& conservative upgrade?
\end{question}

\begin{question}
    The \href{http://flann.super.site/}{FLaNN Group}, specifically the work~\cite{merrill2019sequential,merrill2020formal,merrill2023expressive,strobl2024formal} considers neural networks as automata, and asks the question ``what functions can different neural networks encode?''  Similarly, we consider neural networks as models for logic, and ask the question ``what formulas can different neural networks model?''  These questions are clearly related --- but how, precisely?
\end{question}

\begin{question}
    The FLaNN perspective (neural networks as automata) is one way to characterize the computational power of neural networks.  Can we use neural network models to characterize the \emph{expressive} power of neural networks?  How does this all relate to the computational and descriptive complexity hierarchies?
\end{question}

\section*{The Expressive Power of Neural Network Models}

\subsection*{Basic Setup and Definitions}

My first goal is to compare neural network models against other models.  To make the comparison fair, all models will share the basic multi-modal language
\[
    p \mid \neg \varphi \mid \varphi \land \psi \mid \set{\Box_i}_{i \in \textbf{I}} \hspace*{0.5em} \varphi
\]
where \textbf{I} is some fixed set of indices.  I have in mind that each $\Box_i$ represents a different modality per use-case (e.g. belief vs knowledge), although they could also be used to model a multi-agent setting (e.g. agent 1's belief vs agent 2's belief).

Here are some classes of models I'm interested in:

\paragraph*{Relational (Kripke) Models.} A relational model is $\Model = \langle W, \set{R_i}_{i \in \textbf{I}}, V \rangle$, where 
\begin{itemize}
    \item $W$ is some finite set of worlds (or states)
    \item Each $R_i \subseteq W \times W$ (the accessibility relations)
    \item $V : \textrm{Proposition} \to \mathcal{P}(W)$ (the valuation function)
\end{itemize}
Define $\Rel$ to be the class of all such models, and define $\Relrefl$ to be the class of all such models where each $R_i$ is additionally reflexive and transitive.  The semantics for both classes is given by:
\[
\begin{array}{lcl}
    \Model, w \Vdash p & \mbox{ iff } & w \in V(p)\\
    \Model, w \Vdash \neg \varphi & \mbox{ iff } & \Model, w \not \Vdash \varphi\\
    \Model, w \Vdash \varphi \land \psi & \mbox{ iff } & \Model, w \Vdash \varphi \mbox{ and } \Model, w \Vdash \psi\\
    \Model, w \Vdash \Box_i \varphi & \mbox{ iff } & \mbox{for all } u \mbox{ with } w{R_i}u, \Model, u \Vdash \varphi
\end{array}
\]

\paragraph*{Plausibility Models.}
A plausibility model is $\Model = \langle W, \set{R_i}_{i \in \textbf{I}}, V \rangle$, i.e. the models themselves are just relational models.  As before, I assume that $W$ is finite, and $R_i$ is reflexive and transitive.  The key difference is that we interpret $\Box_i \varphi$ to hold in the best (or most plausible) states satisfying $\varphi$.  Formally, let $\best_{R_i}{(S)} = \set{w \in S \mid \textrm{For all } u \in S, \neg u {R_i} w}$ (the $R_i$-minimal states over $S$).  The new semantics for $\Box_i$ is
\[
\begin{array}{lcl}
    \Model, w \Vdash \Box_i \varphi & \mbox{ iff } & w \in \best_{R_i}{(\semantics{\varphi})}
\end{array}
\]
where $\semantics{\varphi} = \set{u \mid \Model, u \Vdash \varphi}$.  Let $\Plaus$ be the class of all such models.

Any plausibility operator $\Box_i$ picks out a corresponding conditional: $\Box_i \varphi \to \psi$ reads ``the best $\varphi$ are $\psi$,'' which in the KLM tradition is the semantics for the conditional $\varphi \Rightarrow \psi$.

\paragraph*{Neighborhood Models.} A neighborhood model is $\Model = \langle W, \set{f_i}_{i \in \textbf{I}}, V \rangle$, where $W$ and $V$ are as before and each $f_i : W \to \mathcal{P}(\mathcal{P}(W))$ is an accessibility \emph{function}.  The intuition is that $f_i$ maps each state $w$ to the ``formulas'' (sets of states) that hold at $w$.  Define $\Nbhd$ to be the class of all neighborhood models.

The semantics for $\Model \in \Nbhd$ is the same as the previous two, except the $\Box_i$ case is now:
\[
\begin{array}{lcl}
    \Model, w \Vdash \Box_i \varphi & \mbox{ iff } & \semantics{\varphi} \in f_i(w)
\end{array}
\]
where again $\semantics{\varphi} = \set{u \mid \Model, u \Vdash \varphi}$.

\paragraph*{Weighted Neural Network Models.} 
In our work \cite{kisby2024hebbian} so far, we've only defined the neural network state operators $\Prop$ and $\Reach$.  But in principle, we could define other closure operators, each reflecting a different kind of modality or conditional.  I want to characterize what a neural network model can \emph{in principle model}, and for this we need a more general definition.

A neural network model is $\Net = \langle N, \set{E_i}_{i \in \textbf{I}}, \set{W_i}_{i \in \textbf{I}}, \set{A}_{i \in \textbf{I}}, \set{F^i}_{i \in \textbf{I}}, V \rangle$, where
\begin{itemize}
    \item $N$ is a finite nonempty set (the set of neurons)
    \item Each $E_i \subseteq N \times N$ (the edge relations)
    \item $W_i : E_i \to \mathbb{Q}$ (the weights for each edge relation)
    \item $A_i : \mathbb{Q} \to \mathbb{Q}$ (the activation function for each edge relation)
    \item For each index $i$ and each state $S \in \mathcal{P}(N)$, $F^i_S : N \to \mathbb{Q}$ is a function depending only on input $n$, the predecessors of $n$ given by $E_i$, the weights $W_i$, and the activation function $A_i$.
    \item $V : \textrm{Proposition} \to \mathcal{P}(N)$ (the valuation function)
\end{itemize}
I assume that each $A_i$ is a binary step functions.  I also assume that each $F^i$ is binary as well (its codomain is $\set{0, 1}$).  Using the terminology of \cite{merrill2020formal}, this means the net is \emph{saturated}.  Later, I will assume one more constraint on the architecture of these nets.

I've generalized the definition from before in two ways.  First, these neural network models can have multiple kinds of edges (indexed by $i \in \textbf{I}$) connecting the same nodes, along with their weights and a corresponding activation function for each $i$.  I won't use this feature so much --- it's mainly to allow a fair comparison to the other classes of models.

Second, I now allow different kinds of state transitions; each $F^i$ specifies how the net will use the edges, weights, and activation function to transition into the next state.  Specifically, $F^i_S(n)$ gives the activation of $n$ in the next state, given that the current state is $S$.  You can think of $E_i$, $W_i$, and $A_i$ as ``optional parameters'' that $F^i$ may make use of.

\begin{example*}
    Here are some example functions $F^i$ we could pick:
    \begin{enumerate}
        \item $F^i_S(n) = A_i(\sum_{m \in \preds{n}} W_i(m, n) \cdot \bigchi_S(m))$, where $\bigchi_S(m) = 1$ iff $m \in S$ is the indicator function.  This is the usual choice for artificial neural networks.
        \item $F^i_S(n) = 1$ whenever there is some $E_i$-predecessor $m$ of $n$ in $S$.
        \item $F_S(n) = 1$ whenever more than half of the $E_i$-predecessors of $n$ are in $S$.  This is one of the choices that \cite{baltag2019socialnetworks} consider for modelling influence in social networks.
    \end{enumerate}
\end{example*}

Let $\NetModels$ be the class of all neural network models (with one more constraint that I'll say soon).  The semantics for $\Net \in \NetModels$ is a bit more roundabout than for the previous classes.  First, we define a ``next state'' function $\nextstate_i : \mathcal{P}(N) \to \mathcal{P}(N)$ for each $i \in \textbf{I}$ as follows:
\[
    \nextstate_i(S) = S \cup \set{n \mid F^i_S(n) = 1}
\]
Notice that $\nextstate(S)$ is extensive --- once activated, a node will stay activated in the next state.

\begin{postulate*}
    I assume that for all $i \in \textbf{I}$ and all states $S$, $\nextstate_i$ applied to $S$, i.e.
    \[
        S, \nextstate_i(S), \nextstate_i(\nextstate_i(S)), \ldots, \nextstate^k_i(S), \ldots
    \]
    has a least fixed point, and moreover that it is \emph{unique}.  Let $\Closure_i : \mathcal{P}(N) \to \mathcal{P}(N)$ (``closure'') be the function that produces that least fixed point.
\end{postulate*}
This assumption implicitly constrains the allowed neural network architectures: We allow feed-forward nets, as well as certain controlled forms of recurrence.  Characterizing nets that have a unique least fixed point is a big open problem.

\begin{example*}
    Different choices of $F^i_S$ give different interpretations for $\Closure(S)$.  Consider the functions from the previous example.  For (1) we have $\Closure_i(S) = \Prop(S)$, the forward-propagation (or diffusion) of the activation pattern $S$ in a neural network.  (2) gives us $\Closure_i(S) = \Reach(S)$, the nodes graph-reachable from $S$.  Finally, $\Closure_i$ for (3) can be interpreted as the diffusion of an opinion or attitude through a social network (see \cite{baltag2019socialnetworks}).
\end{example*}

I can now state the semantics for $\Net \in \NetModels$:
\[
\begin{array}{lcl}
    \Net, n \Vdash p & \mbox{ iff } & n \in V(p)\\
    \Net, n \Vdash \neg \varphi & \mbox{ iff } & \Net, n \not \Vdash \varphi\\
    \Net, n \Vdash \varphi \land \psi & \mbox{ iff } & \Net, n \Vdash \varphi \mbox{ and } \Model, n \Vdash \psi\\
    \Net, n \Vdash \Diamond_i \varphi & \mbox{ iff } & 
    n \in \Closure_i(\semantics{\varphi})
\end{array}
\]
where $\semantics{\varphi} = \set{n \mid \Net, n \Vdash \varphi}$.  

Any network diffusion operator $\Diamond_i$ picks out a corresponding neural network inference: $\Diamond_i \varphi \leftarrow \psi$ says that on input $\varphi$ the neural network ``answers'' with classification $\psi$.  This is analogous to the way a plausibility operator picks out a conditional (I will say more about this later).

\paragraph*{Dynamic Models.}

I would also like to compare neural network \emph{update} against other model updates --- what kind of updates are neural networks capable of modelling, and how expressive are they in this sense?  We can ``dynamify'' each of the models above using the dynamic epistemic logic trick.  First, we extend our language to include dynamic modal operators:
\[
    p \mid \neg \varphi \mid \varphi \land \psi \mid \set{\Box_i}_{i \in \textbf{I}} \hspace*{0.5em} \varphi \mid \set{\Update{\varphi}_j}_{j \in \textbf{J}} \hspace*{0.5em} \psi
\]
where \textbf{J} is a new set of indices.  As before, the idea is that each $\Update{\varphi}_i$ represents a different update per use-case, although taking $\textbf{I} = \textbf{J}$ these could also be used to model different updates per agent.  

I will define the semantics for dynamic models by example.  First, consider $\Rel$.  For any $\Model \in \Rel$ and $S \in \mathcal{P}(W)$, we can define a variety of dynamic update functions $\set{\UpdateVerbatim_j}_{j \in \textbf{J}}$, where each $\UpdateVerbatim_j : \Rel \times \mathcal{P}(W) \to \Rel$.  A classical example is public announcement, where $\UpdateVerbatim(\langle W, R, V \rangle, S) = \langle W \cap S, R, V \rangle$.  Note that we're using sets $S$ as input rather than formulas, although the choice of one over the other is just my preference.

Also note that these updates don't depend on the current world $w$.  I've read a good bit of \cite{van2011logicaldynamics} (a great book on dynamic epistemic logics), and while Johan includes dependence on $w$ I haven't yet run across an update that uses this extra information.  I'll drop it for now, but if we need it later I can add it back in.

The semantics for $\Model \in \Rel$ over the dynamic language is exactly as before, with an additional case for $\Update{\varphi}_j \psi$.  We just interpret $\Update{\varphi}_j \psi$ as ``$\psi$ holds after updating by $\varphi$'':
\[
\begin{array}{lcl}
    \Model, w \Vdash \Update{\varphi}_j \psi & \mbox{ iff } & \Model^\star_{\semantics{\varphi}}, w \Vdash \psi
\end{array}
\]


% A dynamic relational model is $\Model' = \langle W, \set{R_i}_{i \in \textbf{I}}, \set{\UpdateVerbatim_j}_{j \in \textbf{J}}, V \rangle$, where each $\UpdateVerbatim_j : \Rel \times \mathcal{P}(W) \to \Rel$ is a dynamic update function that produces a new model given a set $S \in \mathcal{P}(W)$.  Given $\Model \in \Rel$ and $S \in \mathcal{P}(W)$, I'll write $\Model_S^{\star_j} = \UpdateVerbatim_j(\Model, S)$ as shorthand.  Define $\Rel^\star$ to be the class of all such models.

% $\Plaus^\star$, $\Nbhd^\star$, and $\NetModels^\star$ are defined similarly --- just extend the model with update functions $\UpdateVerbatim_j$ and the semantics are the same.


% P ] is some dynamic update given by M → M⋆
% P (this is a free variable; the problem will be to find the right update).

\paragraph*{Theories of Model Classes.} 

\begin{definition}
    Let $\Model$ be any model whatsoever with universe $W$ and satisfaction relation $\Vdash$.  $\Model \models \varphi$ if for all $w \in W$, $\Model, w \Vdash \varphi$.
\end{definition}

\begin{definition}
    Let $\Class$ be a class of models, $\Model$ be a model in $\Class$, and $\lang$ be a language.  The \emph{theory} of $\Model$ over $\lang$ is given by $\Theory{\Model} = \set{\varphi \in \lang \mid \Model \models \varphi}$.  The \emph{theory of class $\Class$} over $\lang$ is given by 
    \[
        \Theory{\Class} = \set{\varphi \in \lang \mid \textrm{there is some } \Model \in \Class \textrm{ such that } \Model \models \varphi}
    \]
\end{definition}

\begin{example*}
    Here's an example that's a sort of tutorial for comparing the expressive power of different classes using $\Theory{\Class}$.  Consider relational models $\Rel$ over the static language.  Here are some formulas that are valid in every relational model:
    \begin{itemize}
        \item $\Box_i(\varphi \land \psi) \to (\Box_i \varphi \land \Box_i \psi)$
        \item $(\Box_i \varphi \land \Box_i \psi) \to \Box_i(\varphi \land \psi)$
        \item $\Box_i \top$
    \end{itemize}
    This means that \emph{no} $\Model \in \Rel$ can satisfy their negations (for details, see \cite{pacuit2017neighborhood}, page 8).  But neighborhood models can: Let $\Model = \langle W, f, V \rangle$ be a neighborhood model with $W = \set{a, b, c}$, propositions $\set{p, q}$ with $V(p) = \set{a, b}, V(q) = \set{b, c}$, and $f$ given by
    \[
    \begin{tikzcd}[column sep=1em]
        \set{a, c} & \set{c} & \set{b} & \emptyset & \set{a} & \set{b,c} & \set{b} \\
           &  a \arrow{lu} \arrow{u} \arrow{ru}  &     &  b \arrow{lu} \arrow{u} \arrow{ru}  &    &  c \arrow{lu} \arrow{u} \arrow{ru} &
    \end{tikzcd}
    \]
    For all $w \in W$ we have $\semantics{p} \cap \semantics{q} = \set{b} \in f(w)$, but also $\semantics{p} = \set{a, b} \not \in f(w)$. This means that for all $w$, $\Model, w \not \Vdash \Box(\varphi \land \psi) \to (\Box \varphi \land \Box \psi)$ --- in other words, $\Model \models \neg (\Box(\varphi \land \psi) \to (\Box \varphi \land \Box \psi))$.  Consequently, this formula is in $\Theory{\Nbhd}$, but \emph{not} in $\Theory{\Rel}$.
    
    Moreover, neighborhood models are at least as expressive as relational models: $\Theory{\Rel} \subseteq \Theory{\Nbhd}$.  Let $\varphi \in \lang$ and suppose $\Model \models \varphi$ for some $\Model = \langle W, \set{R_i}_{i \in \textbf{I}}, V \rangle \in \Rel$.  Eric Pacuit in \cite{pacuit2017neighborhood}, page 47 shows how to construct an equivalent neighborhood model:  Let $\Model' = \langle W, \set{f_i}_{i \in \textbf{I}}, V \rangle$, where each $f_i(w) = \set{X \mid \set{v \mid w{R_i}v } \subseteq X}$.  All we need to show is $\Model' \models \varphi$, but we are able to prove something even stronger:
    \begin{proposition}
        For all $\varphi$ and all $w$, $\Model, w \Vdash \varphi$ iff $\Model', w \Vdash \varphi$.
    \end{proposition}
    \begin{proof}
        By induction on $\varphi$.  The key case is $\Box_i \varphi$.  
        
        $(\to)$: Suppose $\Model, w \Vdash \Box_i \varphi$, i.e. for all $u$ with $w{R_i}u$, $\Model, u \Vdash \varphi$.  It follows that $\set{u \mid w{R_i}u } \subseteq \semantics{\varphi}$. By construction of $\Model'$, this means $\semantics{\varphi} \in f_i(w)$, and so $\Model', w \Vdash \Box_i \varphi$.

        $(\leftarrow)$: Now suppose $\Model', w \Vdash \Box_i \varphi$, i.e. $\semantics{\varphi} \in f_i(w)$.  By construction of $\Model'$, we have $\set{u \mid w{R_i}u } \subseteq \semantics{\varphi}$.  So for all $u$ with $w{R_i}u$, $\Model, u \Vdash \varphi$.  This gives us $\Model, w \Vdash \Box_i \varphi$.
    \end{proof}

    Putting everything together, neighborhood models are \emph{strictly} more expressive than relational models:
    \begin{proposition}
        $\Theory{\Rel} \subset \Theory{\Nbhd}$
    \end{proposition}

\end{example*}


\subsection*{Progress So Far}

\subsubsection*{The Static Case.}

We are now set up to answer the first part of Question 1.  We can see how the expressive power of neural network models compares against the rest by arranging their theories in an ``expressivity hierarchy.''  Over the static language (no dynamic operators), we have:
\[
\begin{tikzcd}
    & \Theory{\Nbhd} \arrow[symbol=\supset]{ld} \arrow[symbol=\supset]{rd} & & \\
    \Theory{\Rel} \arrow[symbol=\supset]{rd} & & \Theory{\Plaus} \arrow[symbol={=}]{r} \arrow[symbol=\supset]{ld} & \Theory{\NetModels} \\
    & \Theory{\Relrefl} & &
    % A \arrow[r,symbol=\supseteq] &B \arrow[d] \\
    % & G \arrow[r,symbol=\leq] & H
\end{tikzcd}
\]
These inclusions are all either already known or are folklore.  We already proved $\Theory{\Rel} \subset \Theory{\Nbhd}$ in the example above.  It's easy to see that $\Theory{\Relrefl} \subset \Theory{\Rel}$: Given any $\Model \in \Relrefl$ satisfying $\varphi$, the very same model is $\in \Rel$.  As for the strictness, the negation of the reflexivity axiom $\neg (\Box{\varphi} \to \varphi) \in \Theory{\Rel}$, yet $\neg (\Box{\varphi} \to \varphi) \not \in \Theory{\Relrefl}$.

Neither $\Theory{\Rel}$ nor $\Theory{\Plaus}$ are contained in the other.  On the one hand, nonmonotonicity $\neg (\Box (\varphi \land \psi) \to \Box \varphi \land \Box \psi)$ is satisfiable by $\Model \in \Plaus$ but not by $\Model \in \Rel$.  On the other hand, the negation of the reflexivity axiom $\Box \varphi \to \varphi$ is satisfiable by $\Model \in \Rel$, but reflexivity and transitivity hold in all plausibility models (we could have also taken the corresponding transitive or cumulative axioms).  Because of this, it's more helpful to compare $\Theory{\Plaus}$ with $\Theory{\Relrefl}$.

Now for the more interesting inclusions:

\begin{proposition}
    $\Theory{\Relrefl} \subset \Theory{\Plaus}$
\end{proposition}

\begin{proposition}
    $\Theory{\Plaus} \subset \Theory{\Nbhd}$
\end{proposition}
% \begin{proof}
%     This seems like it ought to be a classical result about neighborhood models, but I can't source it.  If you happen to know of a source for this, let me know.

%     [sorry]
% \end{proof}

\begin{proposition}
    $\Theory{\Plaus} \subseteq \Theory{\NetModels}$
\end{proposition}

\begin{proposition}
    $\Theory{\NetModels} \subseteq \Theory{\Plaus}$
\end{proposition}


% \begin{proof}
%     As for $\Theory{\NetModels} = \Theory{\Plaus}$, Hannes in \cite{leitgeb2001nonmonotonic, leitgeb2003nonmonotonic} proved this for particular classes of plausibility models and networks over a conditional language.  Since my models are somewhat more general, and I use a modal language, I just need to lift his result to the new setting:
%     sorry
% \end{proof}

\paragraph*{What does this say about interpreting neural networks?}
The popular conception of neural networks is that they are ``black boxes'' whose behavior is difficult to understand.  This last fact tells us, however, that neural network inference (classification) corresponds precisely to defeasible conditionals.  When Thomas Icard last visited, he pointed out to me that this intuition is only helpful for describing the net's input/output behavior, but not for explaining the net's hidden states.  Because of this, Thomas thinks that the ``neural network model'' formalism is on the wrong track (or at least, unenlightening), and he decided not to continue working on it.

I take this criticism seriously, but to me the underlying issue is that the hidden states are determined through learning, and we don't yet have a classical intuition for neural network learning.  If we did, then we could understand and track the changes in the hidden states during the learning process, and there would be no need for extraction.  I'll revisit this point at the end of the next section.

\subsubsection*{The Dynamic Case.}

\subsection*{Neural Networks and the Complexity Hierarchy}

[Include big picture relating Chomsky hierarchy, neural networks as automata (Will Merill's work), and descriptive complexity, done in Inkscape]

\subsubsection*{The Dynamic Case.}

[All of this suggests a whole research program --- identify important classes of \emph{dynamic updates} and explore the \emph{dynamic} complexity hierarchy! Say what this means for automata, neural networks, and logic expressivity.  Open problems abound, this is very unexplored territory!]

\printbibliography
\end{document}

