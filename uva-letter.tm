<TeXmacs|2.1.1>

<style|<tuple|letter|termes-font|compact-list>>

<\body>
  <\hide-preamble>
    \;

    <assign|signature|<macro|body|<surround|<vspace*|5fn><left-flush>||<signature*|<arg|body>>>>>

    <assign|closing|<macro|body|<surround|<left-flush>||<signature*|<arg|body>>>>>

    <assign|doc-misc|<macro|body|<doc-title-block|<arg|body>>>>

    <assign|doc-title|<macro|x|<\surround|<vspace*|0.5fn>|<vspace*|0.5fn>>
      <doc-title-block|<font-magnify|1.682|<doc-title-name|<arg|x>>>>
    </surround>>>

    <assign|doc-title-name|<macro|x|<arg|x>>>

    <assign|opening|<macro|body|<vspace|0.33fn><opening*|<arg|body>>>>

    <assign|bibliography-text|<macro|<localize|References>>>
  </hide-preamble>

  <center|<tabular|<tformat|<cwith|1|-1|2|2|cell-halign|c>|<cwith|1|-1|2|2|cell-valign|c>|<twith|table-width|1par>|<twith|table-hmode|exact>|<twith|table-bborder|0.075fn>|<cwith|1|1|2|2|cell-bsep|1fn>|<cwith|3|3|1|-1|cell-lsep|0fn>|<cwith|3|3|1|-1|cell-rsep|0fn>|<cwith|3|3|1|-1|cell-bsep|0fn>|<cwith|3|3|1|-1|cell-tsep|0fn>|<twith|table-tsep|0fn>|<table|<row|<cell|>|<cell|<huge|Caleb
  Schultz Kisby>>|<cell|>>|<row|<cell|>|<cell|<tabular|<tformat|<table|<row|<cell|<with|font-series|bold|Email:>
  <hlink|cckisby@gmail.com|mailto:cckisby@iu.edu>>|<cell|<space|2em>>|<cell|<with|font-series|bold|Website:>
  <hlink|ais-climber.github.io|https://ais-climber.github.io/>>>>>>>|<cell|>>|<row|<cell|>|<cell|>|<cell|>>>>>>

  \;

  <with|font-series|bold|Position:> Postdoctoral Research Position in
  Human(e) AI, University of Amsterdam<vspace|0.5fn>

  <\opening>
    Dear members of the Human(e) AI Steering Board,
  </opening>

  I'm writing to apply for a Postdoc position through your Human(e) AI RPA,
  within the AI and Logic track. I am currently a PhD candidate in Computer
  Science at Indiana University (expected defense: April 2025). My research
  focuses on foundational questions that underlie artificial intelligence
  (AI) and cognition, in particular:

  <\with|par-par-sep|0.1fn>
    <\itemize>
      <item>How should we best integrate symbolic and neural (sub-symbolic)
      systems?

      <item>How can we extract, interpret, and verify the internal beliefs of
      neural networks?

      <item>How powerful and reliable are different learning algorithms, when
      compared to one another?

      <item>Is provably correct AI alignment possible?
    </itemize>
  </with>

  I tend to approach these questions using tools from logic and theoretical
  computer science. But this work is necessarily interdisciplinary in nature,
  so I also borrow and share ideas across many other fields including machine
  learning, philosophy, psychology, linguistics, and neuroscience. As my CV
  shows, I contribute to both mainstream AI conferences as well as to
  interdisciplinary meetings (including the cognitive lunch seminar at IU,
  the LIRa seminar at UvA). For my PhD, I have answered many of these
  questions in a somewhat simplified setting. My long-term goal is to see
  these questions answered for neural networks and learning algorithms that
  are used in practice.

  I have in mind two points of collaboration within your Human(e) AI
  initiative. First, I'm interested in working with the Amsterdam Dynamics
  Group at the ILLC on developing formal logics with the aim of designing
  safe, trustworthy, and interpretable machine learning systems. Actually,
  Sonja Smets encouraged me to apply to this position in the first place\Vwe
  met in January, along with Alexandru Baltag, and realized that my approach
  to modelling dynamics in neural networks bears striking resemblance to her
  work in modelling dynamics in <with|font-shape|italic|social> networks. On
  a personal note, conversations with Sonja and Alexandru, as well as the
  work of the Dynamics group as a whole, have had a big influence on my
  development as a researcher. I would be delighted to have the opportunity
  to work with this team.

  Outside of AI and logic, I'm interested in learning from and sharing ideas
  with with the Epistemological and Ethical \<#2018\>Explainable AI\<#2019\>
  team. As I explain in my proposal, I plan on implementing agents that obey
  constraints <with|font-shape|italic|before and after> they learn from data.
  A crucial step here is to come up with realistic, human-interpretable
  ethical constraints and formalize them in the logic language. As far as I'm
  concerned this requires the expertise of a team like yours, and I would be
  very happy to be in a position to work together on this.

  Thank you for your time and consideration. If you have any further
  questions, I'm available at the email above, as well as over Zoom.

  <\with|par-par-sep|0.3333fn>
    <\closing>
      <right-aligned|Caleb Schultz Kisby>
    </closing>
  </with>
</body>

<\initial>
  <\collection>
    <associate|bg-color|#f9f5d7>
    <associate|font-base-size|12>
    <associate|item-vsep|<macro|0fn>>
    <associate|math-font|math-termes>
    <associate|page-bot|1in>
    <associate|page-even|1in>
    <associate|page-even-footer|<htab|5mm>>
    <associate|page-even-header|>
    <associate|page-height|auto>
    <associate|page-medium|paper>
    <associate|page-odd|1in>
    <associate|page-odd-footer|<htab|5mm>>
    <associate|page-odd-header|>
    <associate|page-right|1in>
    <associate|page-screen-margin|false>
    <associate|page-top|0.75in>
    <associate|page-type|letter>
    <associate|page-width|auto>
    <associate|par-par-sep|0.5fn>
    <associate|par-sep|0.3fn>
  </collection>
</initial>